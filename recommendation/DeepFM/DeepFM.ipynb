{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.11</td>\n",
       "      <td>8.21</td>\n",
       "      <td>8.31</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.12</td>\n",
       "      <td>8.22</td>\n",
       "      <td>8.32</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.13</td>\n",
       "      <td>8.23</td>\n",
       "      <td>8.33</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.14</td>\n",
       "      <td>8.24</td>\n",
       "      <td>8.34</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2  feature3 feature4 feature5 feature6\n",
       "0      8.11      8.21      8.31        A        A        A\n",
       "1      8.12      8.22      8.32        A        E        E\n",
       "2      8.13      8.23      8.33        B        B        B\n",
       "3      8.14      8.24      8.34        C        C        C"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config\n",
    "\n",
    "latent_vector_size = 3\n",
    "mlp_layers = [16,4,1]\n",
    "embedding_dim = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# example data\n",
    "\n",
    "col_info_list = [\n",
    "    {'col_name': 'feature1','col_type': 'continuous'},\n",
    "    {'col_name': 'feature2','col_type': 'continuous'},\n",
    "    {'col_name': 'feature3','col_type': 'continuous'},\n",
    "    {'col_name': 'feature4','col_type': 'categorical'},\n",
    "    {'col_name': 'feature5','col_type': 'categorical'},\n",
    "    {'col_name': 'feature6','col_type': 'categorical'},\n",
    "]\n",
    "\n",
    "Y = [1, 0 , 0, 1]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'feature1': [8.11, 8.12, 8.13, 8.14],\n",
    "    'feature2': [8.21, 8.22, 8.23, 8.24],\n",
    "    'feature3': [8.31, 8.32, 8.33, 8.34],\n",
    "    'feature4': ['A', 'A', 'B', 'C'],\n",
    "    'feature5': ['A', 'E', 'B', 'C'],\n",
    "    'feature6': ['A', 'E', 'B', 'C'],\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert Y to tensor\n",
    "\n",
    "Y = torch.tensor(Y, dtype=torch.float)\n",
    "Y = Y.view(-1, 1)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'col_name': 'feature1', 'col_type': 'continuous'},\n",
       " {'col_name': 'feature2', 'col_type': 'continuous'},\n",
       " {'col_name': 'feature3', 'col_type': 'continuous'},\n",
       " {'col_name': 'feature4', 'col_type': 'categorical', 'num_unique': 3},\n",
       " {'col_name': 'feature5', 'col_type': 'categorical', 'num_unique': 4},\n",
       " {'col_name': 'feature6', 'col_type': 'categorical', 'num_unique': 4}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate number of unique values in each categorical column\n",
    "\n",
    "for col_info in col_info_list:\n",
    "        if col_info['col_type']=='categorical':\n",
    "                col_info['num_unique'] = df[col_info['col_name']].nunique()\n",
    "col_info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.1100, 8.2100, 8.3100],\n",
       "        [8.1200, 8.2200, 8.3200],\n",
       "        [8.1300, 8.2300, 8.3300],\n",
       "        [8.1400, 8.2400, 8.3400]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize continuous columns\n",
    "\n",
    "continous_columns = [col_info['col_name'] for col_info in col_info_list if col_info['col_type']=='continuous']\n",
    "x_continuous = torch.tensor(df[continous_columns].values, dtype=torch.float32)\n",
    "x_continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 3, 3],\n",
       "        [1, 1, 1],\n",
       "        [2, 2, 2]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label encode categorical columns\n",
    "\n",
    "x_categorical = None\n",
    "for col_info in col_info_list:\n",
    "    if col_info['col_type']=='categorical':\n",
    "        # load\n",
    "        col_data = df[col_info['col_name']]\n",
    "        num_unique_val = col_info['num_unique']\n",
    "        label_encoder = LabelEncoder()\n",
    "        \n",
    "        # transform\n",
    "        col_data = label_encoder.fit_transform(col_data)\n",
    "        col_data = torch.tensor(col_data, dtype=torch.long)\n",
    "        col_data = col_data.view(1, -1)\n",
    "\n",
    "        # concat\n",
    "        if x_categorical is None:\n",
    "            x_categorical = col_data\n",
    "        else:\n",
    "            x_categorical = torch.cat((x_categorical, col_data), dim=0)\n",
    "            \n",
    "        # save for inference\n",
    "        col_info['label_encoder'] = label_encoder\n",
    "\n",
    "x_categorical = x_categorical.T\n",
    "x_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep(nn.Module):\n",
    "    def __init__(self, num_features, mlp_layers):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_features: number of features\n",
    "            mlp_layers : list of hidden layer sizes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear_layers = []\n",
    "        for i in range(len(mlp_layers)):\n",
    "            if i==0:\n",
    "                self.linear_layers.append(nn.Linear(num_features, mlp_layers[i]))\n",
    "            else:\n",
    "                self.linear_layers.append(nn.Linear(mlp_layers[i-1], mlp_layers[i]))\n",
    "            self.linear_layers.append(nn.ReLU())\n",
    "        self.linear_layers = nn.ModuleList(self.linear_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.linear_layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FM(nn.Module):\n",
    "    def __init__(self, num_features=None, latent_vector_size=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_features : number of features\n",
    "            latent_vector_size : FM latent vector size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.latent_vector = nn.Parameter(torch.randn(num_features, latent_vector_size).float(), requires_grad=True)\n",
    "        self.lin = nn.Linear(num_features, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out_1 = torch.matmul(x, self.latent_vector).pow(2).sum(1, keepdim=True) #S_1^2\n",
    "        out_2 = torch.matmul(x.pow(2), self.latent_vector.pow(2)).sum(1, keepdim=True) # S_2\n",
    "        \n",
    "        out_inter = 0.5*(out_1 - out_2)\n",
    "        out_lin = self.lin(x)\n",
    "        out = out_inter + out_lin\n",
    "        out = self.activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFM(nn.Module):\n",
    "    def __init__(self, col_info_list, latent_vector_size, mlp_layers, embedding_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n: number of features\n",
    "            mlp_layers : list of hidden layer sizes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # for embedding the categorical columns\n",
    "        self.embedding_list = [nn.Embedding(col_info['num_unique'], embedding_dim) \n",
    "                               for col_info in col_info_list \n",
    "                               if col_info['col_type']=='categorical']\n",
    "        self.embedding_list = nn.ModuleList(self.embedding_list)\n",
    "        \n",
    "        # calculate the total feature size\n",
    "        num_categorical_columns = len(self.embedding_list)\n",
    "        num_continuous_columns = len(col_info_list) - num_categorical_columns\n",
    "        feature_size = num_categorical_columns * embedding_dim + num_continuous_columns\n",
    "        \n",
    "        # use it to create FM and Deep model\n",
    "        self.FM_model = FM(feature_size, latent_vector_size)\n",
    "        self.Deep_model = Deep(feature_size, mlp_layers)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x_categorical, x_continuous):\n",
    "        # embed the categorical columns\n",
    "        x_categorical_embedded = [embedding(x_categorical[:, i]) for i, embedding in enumerate(self.embedding_list)]\n",
    "        x_categorical_embedded = torch.cat(x_categorical_embedded, dim=1)\n",
    "        \n",
    "        # concatenate categorical and continuous columns\n",
    "        x = torch.cat([x_categorical_embedded, x_continuous], dim=1)\n",
    "        \n",
    "        # get output from FM and Deep part\n",
    "        y_fm = self.FM_model(x)\n",
    "        y_deep = self.Deep_model(x)\n",
    "        \n",
    "        # Add both and sigmoid\n",
    "        x = self.activation(y_fm + y_deep)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7311],\n",
       "        [0.5000],\n",
       "        [0.7308],\n",
       "        [0.7311]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize and check output model\n",
    "model = DeepFM(col_info_list, latent_vector_size, mlp_layers, embedding_dim)\n",
    "out = model(x_categorical, x_continuous)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6580270528793335\n",
      "Epoch 1, Loss: 0.6561570763587952\n",
      "Epoch 2, Loss: 0.6456461548805237\n",
      "Epoch 3, Loss: 0.6070890426635742\n",
      "Epoch 4, Loss: 0.5439338684082031\n",
      "Epoch 5, Loss: 0.5108123421669006\n",
      "Epoch 6, Loss: 0.5044958591461182\n",
      "Epoch 7, Loss: 0.5034641623497009\n",
      "Epoch 8, Loss: 0.5032670497894287\n",
      "Epoch 9, Loss: 0.5032221674919128\n"
     ]
    }
   ],
   "source": [
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    output = model(x_categorical, x_continuous)\n",
    "    loss = criterion(output, Y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fb_recom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0403c28f0deb655878eaa0ad39584693969f692fca4a4a81ddeb591433e03a63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
